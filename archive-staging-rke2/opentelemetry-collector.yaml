# manually install (after installing the operator through helm)
# kubectl apply -f opentelemetry-collector.yaml
---
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: simplest
  namespace: default
spec:
  # mode: deployment  # default, as a standalone application (managed like other apps)
  mode: daemonset   # run as agent in kubernetes node (so each node has 1 collector)
  # mode: statefulset # like other too but names of pods becomes predictable
  # replicas: 3
  # mode: sidecar       # running alongside the applications to monitor
  #                     # (the application configurations must be slightly adapted)

  # Required to let access to the filelog receiver
  volumeMounts:
    - mountPath: /var/log/pods
      name: varlogpods
  volumes:
    - name: varlogpods
      hostPath:
        path: /var/log/pods
        type: Directory

  config: |
    receivers:
      filelog:
        include:
          # FIXME: as a tryout, limit to only loaders' logs
          - /var/log/pods/swh_loader-bzr*/*/*.log
        exclude:
          # Exclude logs from all containers named otel-collector
          - /var/log/pods/*/otel-collector/*.log
        start_at: beginning
        include_file_path: true
        include_file_name: false
        operators:
        # - type: json_parser
        #   parse_to: body
        # - type: time_parser
        #   parse_from: body.time
        #   layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        # second tryout
          # Find out which format is used by kubernetes
          # - id: get-format
          #   type: router
          #   routes:
          # #     - output: parser-json
          # #       expr: '$$record matches "^\\{"'
          # #     - output: parser-crio
          # #       expr: '$$record matches "^[^ Z]+ "'
          #     - output: parser-containerd
          #       expr: '$$record matches "^[^ Z]+Z"'
          # Parse Docker format
          # - id: parser-json
          #   type: json_parser
          #   output: extract_metadata_from_filepath
          #   timestamp:
          #     parse_from: time
          #     layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          # Parse CRI-O format
          # - id: parser-crio
          #   type: regex_parser
          #   regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$'
          #   output: extract_metadata_from_filepath
          #   timestamp:
          #     parse_from: time
          #     layout_type: gotime
          #     layout: '2006-01-02T15:04:05.000000000-07:00'
          # # Parse CRI-Containerd format
          - id: parser-containerd
            type: regex_parser
            regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$'
            # output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          # Extract metadata from file path
          # - id: extract_metadata_from_filepath
          #   type: regex_parser
          #   regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<run_id>\d+)\.log$'
          #   parse_from: attributes.log.file.path
          # Move out attributes to Attributes
          # - type: metadata
          #   attributes:
          #     stream: 'EXPR($.stream)'
          #     k8s.container.name: 'EXPR($.container_name)'
          #     k8s.namespace.name: 'EXPR($.namespace)'
          #     k8s.pod.name: 'EXPR($.pod_name)'
          #     run_id: 'EXPR($.run_id)'
          #     k8s.pod.uid: 'EXPR($.uid)'
          # Clean up log record
          # - id: clean-up-log-record
          #   type: restructure
          #   ops:
          #     - remove: logtag
          #     - remove: stream
          #     - remove: container_name
          #     - remove: namespace
          #     - remove: pod_name
          #     - remove: run_id
          #     - remove: uid

      otlp:
        protocols:
          grpc:
          http:

    exporters:
      elasticsearch/log:
        # can be replaced by using the env variable ELASTICSEARCH_URL
        endpoints:
          - http://esnode1.internal.softwareheritage.org:9200
          - http://esnode2.internal.softwareheritage.org:9200
          - http://esnode3.internal.softwareheritage.org:9200
        api_key:
        logs_index: staging-logs

    service:
      pipelines:
        logs:
          receivers: [otlp,filelog]
          processors: []
          exporters: [elasticsearch/log]
