# Install the helm chart
# helm {install|upgrade} opentelemetry-collector open-telemetry/opentelemetry-collector --values ./opentelemetry-collector-values.yaml

mode: daemonset
presets:
  # Configures the collector to collect logs.
  # Adds the filelog receiver to the logs pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  logsCollection:
    enabled: false
  # Configures the Kubernetes Processor to add Kubernetes metadata.
  # Adds the k8sattributes processor to all the pipelines
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = daemonset.
  kubernetesAttributes:
    enabled: true
  # Configures the collector to collect host metrics.
  # Adds the hostmetrics receiver to the metrics pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  hostMetrics:
    enabled: false

extraEnvs:
- name: KUBE_NODE_NAME
  valueFrom:
    fieldRef:
      apiVersion: v1
      fieldPath: spec.nodeName

extraVolumes:
- name: varlogpods
  hostPath:
    path: /var/log/pods
    type: Directory

extraVolumeMounts:
- mountPath: /var/log/pods
  name: varlogpods

resources:
  limits:
    cpu: 256m
    memory: 2Gi

# The pod monitor by default scrapes the metrics port.
# The metrics port needs to be enabled as well.
podMonitor:
  enabled: true

ports:
  # The metrics port is disabled by default. However you need to enable the port
  # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor
  # (podMonitor.enabled).
  metrics:
    enabled: true

config:
  exporters:
    # Generalize the default logging (needs to be activated in the exporters)
    # logging/debug:
    #   loglevel: debug
    elasticsearch/swh-log:
      # can be replaced by using the env variable ELASTICSEARCH_URL
      endpoints:
        - http://esnode1.internal.softwareheritage.org:9200
        - http://esnode2.internal.softwareheritage.org:9200
        - http://esnode3.internal.softwareheritage.org:9200
        - http://esnode7.internal.softwareheritage.org:9200
      # logs_index: logs-swh-staging
      logs_index: staging-logs
      # Does not work, fail to parse the configmap error
      # retry_on_failure:
      #   enabled: true
      timeout: 10s
    elasticsearch/system-log:
      # can be replaced by using the env variable ELASTICSEARCH_URL
      endpoints:
        - http://esnode1.internal.softwareheritage.org:9200
        - http://esnode2.internal.softwareheritage.org:9200
        - http://esnode3.internal.softwareheritage.org:9200
        - http://esnode7.internal.softwareheritage.org:9200
      # logs_index: logs-system-staging
      logs_index: staging-system-logs
      timeout: 10s

  extensions:
    # with port-forward, allows to display the pipeline status to see what's been
    # deployed
    zpages:
      endpoint: "0.0.0.0:8889"
    # The health_check extension is mandatory for this chart. Without the health_check
    # extension the collector will fail the readiness and liveliness probes. The
    # health_check extension can be modified, but should never be removed.
    health_check: {}

  receivers:
    filelog/system:
      include:
        - /var/log/pods/*/*/*.log
      exclude:
        # Exclude 'swh*' namespaced logs
        - /var/log/pods/swh*_*/*/*.log
      start_at: beginning
      include_file_path: true
      include_file_name: false
      multiline:
        # as of now, starts as a date pattern (see parser-containerd below)
        line_start_pattern: '^[^ Z]+Z'
      operators:
      # Find out which log format is used to route it to proper parsers
      # Extract metadata from file path
      - id: extract_metadata_from_filepath
        type: regex_parser
        regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{32,36})\/(?P<container_name>[^\._]+)\/(?P<run_id>\d+)\.log$'
        parse_from: attributes["log.file.path"]
        parse_to: resource
      # Parse CRI-Containerd format
      - id: parser-containerd
        type: regex_parser
        regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr)( (?P<logtag>[^ ]*) (?P<message>.*)|.*)$'
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%dT%H:%M:%S.%LZ'
      # e.g. redis logs are "mostly" json, but no the ts entry is a timestamp that's
      # not adequately parsed. Type:"mapper_parsing_exception", Reason:"failed to
      # parse field [Attributes.ts] of type [date] in document...
      # - id: parser-json-message
      #   type: json_parser
      #   parse_from: attributes['message']
      #   parse_to: attributes
      #   if: attributes.message matches "^\\{"

    filelog/swh:
      include:
        # Only keep 'swh*' namespaces
        - /var/log/pods/swh*_*/*/*.log
      start_at: beginning
      include_file_path: true
      include_file_name: false
      multiline:
        # as of now, starts as a date pattern (see parser-containerd below)
        line_start_pattern: '^[^ Z]+Z'
      operators:
      # Find out which log format is used to route it to proper parsers
      # Extract metadata from file path
      - id: extract_metadata_from_filepath
        type: regex_parser
        regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<run_id>\d+)\.log$'
        parse_from: attributes["log.file.path"]
        parse_to: resource
      # Parse CRI-Containerd format
      - id: parser-containerd
        type: regex_parser
        regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr)( (?P<logtag>[^ ]*) (?P<message>.*)|.*)$'
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%dT%H:%M:%S.%LZ'
      # then parse the json formatted message if any
      - id: parser-json-message
        type: json_parser
        parse_from: attributes['message']
        parse_to: attributes
        if: attributes.stream == 'stdout' && attributes.message matches "^\\{"
      # Those were an attempt to inline the json further but entries 'data.kwargs' and
      # 'return_value' are python dict and not json so we cannot parse them.
      # - id: parser-json-kwargs
      #   type: json_parser
      #   parse_from: attributes.data.kwargs
      #   parse_to: attributes
      #   if: attributes.stream == 'stdout' && attributes.data?.kwargs != nil
      # - id: parser-json-return-value
      #   type: json_parser
      #   parse_from: attributes.return_value
      #   parse_to: attributes
      #   if: attributes.stream == 'stdout' && attributes?.return_value != nil
      # This deals with basic key=value logs (it's not able to deal with "multi"
      # values those like key="this is a value" though, so prometheus, memcached logs
      # are not parsed so far)
      # - id: parse-key-value-message
      #   type: key_value_parser
      #   delimiter: "="
      #   pair_delimiter: " "
      #   parse_from: attributes['message']
      #   parse_to: attributes
      #   if: attributes.message matches "^ts="

    # otlp:
    #   protocols:
    #     grpc:
    #       endpoint: ${MY_POD_IP}:4317
    #     http:
    #       endpoint: ${MY_POD_IP}:4318

  processors:
    resource:
      attributes:
        - key: k8s.pod.name
          from_attribute: pod_name
          action: upsert
    k8sattributes:
      filter:
        node_from_env_var: KUBE_NODE_NAME
      passthrough: false
      extract:
        metadata:
          # from https://opentelemetry.io/docs/reference/specification/resource/semantic_conventions/k8s/
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.daemonset.name
          - k8s.job.name
          - k8s.cronjob.name
          # Desired properties (but not working for now
          # 2023/04/26 08:54:58 collector server run finished with error: failed to
          # build pipelines: failed to create "k8sattributes" processor, in pipeline
          # "logs/system": "k8s.cluster.name" (or "deployment.environment" )is not a
          # supported metadata field
          # - k8s.cluster.name
          # - deployment.environment
      pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
        - sources:
          - from: connection
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
    batch:
      # for debug
      # send_batch_size: 1
      send_batch_size: 10
    # If set to null, will be overridden with values based on k8s resource limits
    memory_limiter: null
    # memory_limiter:
    #   check_interval: 60s
    #   limit_percentage: 0
    #   limit_mib: 0
    #   spike_limit_percentage: 25
    attributes/insert:
      actions:
      - key: environment
        value: staging
        action: insert
      - key: cluster
        value: archive-staging-rke2
        action: insert
    attributes/clean-records:
      actions:
      - key: time
        action: delete
      - key: logtag
        action: delete
      - key: log
        action: delete
      - key: log.keyword
        action: delete
      - key: log.file.path
        action: delete
      - key: log.value
        action: delete

  service:
    telemetry:
      metrics:
        address: ${MY_POD_IP}:8888
      # logs:
      #   level: "debug"
    extensions:
      - zpages
      - health_check
      - memory_ballast

    pipelines:
      logs/system:
        receivers:
          # - otlp
          - filelog/system
        processors:
          - memory_limiter
          - batch
          - resource
          - k8sattributes
          - attributes/insert
          - attributes/clean-records
        exporters:
          - elasticsearch/system-log
      logs/swh:
        receivers:
          # - otlp
          - filelog/swh
        processors:
          - memory_limiter
          - batch
          - resource
          - k8sattributes
          - attributes/insert
          - attributes/clean-records
        exporters:
          - elasticsearch/swh-log
      # inhibit pipelines
      logs: null
      metrics: null
      traces: null
